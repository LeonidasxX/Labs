{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ab422e-666d-4e75-a38b-631b9bac466f",
   "metadata": {},
   "source": [
    "Sample Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95177fc7-1baa-4848-9a32-542dbd32eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5682248c-12c7-4a69-9a5b-edd827f3bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor(3.5000, requires_grad=True)\n",
      "y = x*x:  tensor(12.2500, grad_fn=<MulBackward0>)\n",
      "z = 2*y + 3:  tensor(27.5000, grad_fn=<AddBackward0>)\n",
      "Working out gradients dz/dx\n",
      "Gradient at x = 3.5:  tensor(14.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.5, requires_grad=True)\n",
    "y = x*x\n",
    "z = 2*y + 3\n",
    "print(\"x: \",x)\n",
    "print(\"y = x*x: \", y)\n",
    "print(\"z = 2*y + 3: \", z)\n",
    "\n",
    "#working out gradients\n",
    "z.backward()\n",
    "print(\"Working out gradients dz/dx\")\n",
    "\n",
    "#gradient at x = 3.5\n",
    "print(\"Gradient at x = 3.5: \", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26579c00-7036-42f6-9055-e9f6a8f0a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch's f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([1.0],requires_grad=True)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print('Analytical f\\'(x):', fp(x))\n",
    "print('PyTorch\\'s f\\'(x):', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266b8352-649e-4875-97b3-1c9fc7c6f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0])\n",
    "x.requires_grad_(True)\n",
    "\n",
    "y = x**2 + 5\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4ae2b8-c8f2-4a58-813e-ea4d8cc730af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient:  tensor([4.])\n",
      "Analytical gradient:  tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward() # dy/dx\n",
    "print('PyTorch gradient: ', x.grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dy_dx = 2*x\n",
    "\n",
    "print('Analytical gradient: ', dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84832cb0-1f2c-478d-ad93-082c2aecad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autograd:  0.10499356687068939\n",
      "manual:  0.1049935854035065\n"
     ]
    }
   ],
   "source": [
    "def grad_sigmoid_manual(x):\n",
    "    \"\"\"Implements the gradient of the logistic sigmoid function\n",
    "    #sigma(x) = 1 / (1 + e^{-x})\n",
    "    \"\"\"\n",
    "    # forward pass keeping track of intermediate values for use in the backward pass\n",
    "    a = -x\n",
    "    b = np.exp(a)\n",
    "    c = 1 + b\n",
    "    s = 1.0/c\n",
    "    #backward pass\n",
    "    dsdc = (-1.0/(c**2))\n",
    "    dsdb = dsdc * 1\n",
    "    dsda = dsdb * np.exp(a)\n",
    "    dsdx = dsda * (-1)\n",
    "    return dsdx\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1.0 / (1.0 + torch.exp(-x))\n",
    "    return y\n",
    "\n",
    "input_x = 2.0\n",
    "x = torch.tensor(input_x).requires_grad_(True)\n",
    "y = sigmoid(x)\n",
    "y.backward()\n",
    "\n",
    "# Compare the results of manual and automatic gradient functions\n",
    "print('autograd: ', x.grad.item())\n",
    "print('manual: ', grad_sigmoid_manual(input_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c884da-366b-40b6-917a-3247407a2ecc",
   "metadata": {},
   "source": [
    "Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e6503d-1734-4e20-a052-ea69ebf552c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor(8., grad_fn=<AddBackward0>)\n",
      "y =  tensor(29., grad_fn=<AddBackward0>)\n",
      "z =  tensor(103., grad_fn=<AddBackward0>)\n",
      "Gradient dz/da at a = 1 and b = 2:  tensor(34.)\n",
      "Gradient dz/db at a = 1 and b = 2:  tensor(114.)\n",
      "Manually=:  (34.0, 114.0)\n"
     ]
    }
   ],
   "source": [
    "input_a = 1.0\n",
    "input_b = 2.0\n",
    "\n",
    "a = torch.tensor(input_a).requires_grad_(True)\n",
    "b = torch.tensor(input_b).requires_grad_(True)\n",
    "\n",
    "x = 2*a + 3*b\n",
    "y = 5*a*a + 3*b*b*b\n",
    "z = 2*x + 3*y\n",
    "\n",
    "def manually(a,b):\n",
    "    dzda = 2*2 + 3*5*2*a\n",
    "    dzdb = 2*3 + 3*3*3*b*b\n",
    "    return(dzda, dzdb)\n",
    "\n",
    "print(\"x = \", x)\n",
    "print(\"y = \", y)\n",
    "print(\"z = \", z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('Gradient dz/da at a = 1 and b = 2: ', a.grad)\n",
    "print('Gradient dz/db at a = 1 and b = 2: ', b.grad)\n",
    "\n",
    "print(\"Manually=: \", manually(input_a, input_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9bbdcac-fdd2-4e3c-b96e-07b631400d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(1.)\n",
      "3.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "def manual(w, b, x):\n",
    "    u = w * x\n",
    "    v = u + b\n",
    "    a = max(0.0, v)\n",
    "\n",
    "    if a == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        dadv = 1.\n",
    "        dadu = 1.\n",
    "        dadb = 1.\n",
    "        dadw = x\n",
    "        return dadw, dadb\n",
    "\n",
    "input_x = 3.0\n",
    "input_w = 4.0\n",
    "input_b = 5.0\n",
    "\n",
    "x = torch.tensor(input_x)\n",
    "w = torch.tensor(input_w).requires_grad_(True)\n",
    "b = torch.tensor(input_b).requires_grad_(True)\n",
    "\n",
    "u = w*x; u.retain_grad()\n",
    "v = u + b; v.retain_grad()\n",
    "relu = torch.nn.ReLu();\n",
    "a = relu(v); a.retain_grad()\n",
    "\n",
    "a.backward(retain_graph = False)\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "grad_w, grad_b = manual(input_w, input_b, input_x)\n",
    "print(grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9960e0d3-95ee-4993-be87-b4d9e0a29c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0123)\n",
      "tensor(0.0025)\n",
      "0.012332546456800243 0.0024665092913600485\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "from torch.autograd import grad\n",
    "\n",
    "def sigmoid_grad_manual(x):\n",
    "    a = -x\n",
    "    b = np.exp(a)\n",
    "    c = 1+b\n",
    "    e = 1.0/c\n",
    "\n",
    "    dedc = -1.0/(c**2)\n",
    "    dedb = dedc*1\n",
    "    deda = dedb*np.exp(a)\n",
    "    dedx = deda*(-1)\n",
    "\n",
    "    return dedx\n",
    "\n",
    "def manual(w,b,x):\n",
    "    u = w*x\n",
    "    v = u + b\n",
    "    a = sigmoid(v)\n",
    "\n",
    "    dadv = sigmoid_grad_manual(v)\n",
    "    dadu = dadv*1\n",
    "    dadb = dadv*1\n",
    "    dadw = dadu*x\n",
    "    return dadw,dadb\n",
    "\n",
    "\n",
    "inp_x = 5.0\n",
    "inp_w = 1.0\n",
    "inp_b = 1.0\n",
    "\n",
    "x = torch.tensor(inp_x)\n",
    "w = torch.tensor(inp_w,requires_grad = True)\n",
    "b = torch.tensor(inp_b,requires_grad = True)\n",
    "\n",
    "u = w*x\n",
    "v = u + b\n",
    "sigmoidal = torch.nn.Sigmoid()\n",
    "a = sigmoidal(v)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "grad_w,grad_b = manual(inp_w,inp_b,inp_x)\n",
    "print(grad_w,grad_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35dbe33d-55c7-40a4-bca1-835878507ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:  tensor(-3.)\n",
      "Analytical:  -3.0\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "def f(x):\n",
    "    return np.exp((-x**2) -2*x - np.sin(x))\n",
    "\n",
    "def torchF(x):\n",
    "    return torch.exp((-x**2) - 2*x - torch.sin(x))\n",
    "\n",
    "def fDiff(x):\n",
    "    return f(x) * ((-2*x) -2 - np.cos(x))\n",
    "\n",
    "input_x = 0.0\n",
    "x = torch.tensor(input_x).requires_grad_(True)\n",
    "y = torchF(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"PyTorch: \", x.grad)\n",
    "print(\"Analytical: \", fDiff(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47ad185-fe05-48b3-adc8-2d96b0e7af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:  tensor(326.)\n",
      "Analytical:  326.0\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "def f(x):\n",
    "    return 8*(x**4) + 3*x**3 + 7*x**2 + 6*x + 3\n",
    "\n",
    "def fDiff(x):\n",
    "    return 32*x**3 + 9*x**2 + 14*x + 6\n",
    "\n",
    "input_x = 2.0\n",
    "x = torch.tensor(input_x).requires_grad_(True)\n",
    "y = f(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"PyTorch: \", x.grad)\n",
    "print(\"Analytical: \", fDiff(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09351a3b-c922-451e-9568-641ee589ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:  tensor(0.0002)\n",
      "PyTorch:  tensor(0.0021)\n",
      "PyTorch:  tensor(7.5769e-05)\n",
      "e:  tensor(0.0003)\n",
      "d:  tensor(2.6731e-06)\n",
      "c:  tensor(1.0692e-05)\n",
      "b:  tensor(-0.0021)\n",
      "a:  tensor(7.5769e-05)\n",
      "Forward Pass: \n",
      "a:  4.0\n",
      "b:  0.1411200080598672\n",
      "c:  28.344669582948747\n",
      "d:  113.37867833179499\n",
      "e:  4.739514683356544\n",
      "f:  0.9998471354956351\n",
      "df_de:  0.00030570564117315957\n",
      "de_dd:  0.008742888225191356\n",
      "dd_dc:  4.0\n",
      "dc_db:  -200.85507344163494\n",
      "dc_da:  7.086167395737187\n",
      "da_dx:  2\n",
      "db_dy:  -0.9899924966004454\n",
      "xx:  0.00015151644546128608\n",
      "yy:  0.002125852261213157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.002125852261213157, 0.00015151644546128608)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "def manual(x, y, z):\n",
    "    a = 2 ** x\n",
    "    b = np.sin(y)\n",
    "    c = a / b\n",
    "    d = c * z\n",
    "    e = np.log(d + 1)\n",
    "    f = np.tanh(e)\n",
    "    print(\"Forward Pass: \")\n",
    "    print(\"a: \", a)\n",
    "    print(\"b: \", b)\n",
    "    print(\"c: \", c)\n",
    "    print(\"d: \", d)\n",
    "    print(\"e: \", e)\n",
    "    print(\"f: \", f)\n",
    "\n",
    "    df_de = (1 - np.tanh(e)**2)\n",
    "    de_dd = 1 / (d+1)\n",
    "    dd_dc = z\n",
    "    dc_da = 1 / b\n",
    "    da_dx = 2\n",
    "    dc_db = -a / b**2\n",
    "    db_dy = np.cos(y)\n",
    "    print(\"df_de: \", df_de)\n",
    "    print(\"de_dd: \", de_dd)\n",
    "    print(\"dd_dc: \", dd_dc)\n",
    "    print(\"dc_db: \", dc_db)\n",
    "    print(\"dc_da: \", dc_da)\n",
    "    print(\"da_dx: \", da_dx)\n",
    "    print(\"db_dy: \", db_dy)\n",
    "\n",
    "    yy = df_de * de_dd * dd_dc * dc_db * db_dy\n",
    "    xx = df_de * de_dd * dd_dc * dc_da * da_dx\n",
    "    print(\"xx: \", xx)\n",
    "    print(\"yy: \", yy)\n",
    "    return (yy, xx)\n",
    "\n",
    "input_x = 2.0\n",
    "input_y = 3.0\n",
    "input_z = 4.0\n",
    "\n",
    "x = torch.tensor(input_x).requires_grad_(True)\n",
    "y = torch.tensor(input_y).requires_grad_(True)\n",
    "z = torch.tensor(input_z).requires_grad_(True)\n",
    "\n",
    "a = 2 ** x; a.retain_grad()\n",
    "b = torch.sin(y); b.retain_grad()\n",
    "c = a / b; c.retain_grad()\n",
    "d = c * z; d.retain_grad()\n",
    "e = torch.log(d + 1); e.retain_grad()\n",
    "f = torch.tanh(e)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(\"PyTorch: \", x.grad)\n",
    "print(\"PyTorch: \", y.grad)\n",
    "print(\"PyTorch: \", z.grad)\n",
    "print(\"e: \",e.grad)\n",
    "print(\"d: \",d.grad)\n",
    "print(\"c: \",c.grad)\n",
    "print(\"b: \",b.grad)\n",
    "print(\"a: \",a.grad)\n",
    "manual(input_x, input_y, input_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132af24-1c65-4c94-bd32-07f0f0e8ae75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
